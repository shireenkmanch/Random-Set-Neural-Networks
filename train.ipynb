{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 18:28:09.190145: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-22 18:28:09.229809: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-22 18:28:09.915633: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import  Model\n",
    "import numpy as np\n",
    "\n",
    "from data.id_dataloader import load_cifar10, load_intel_image, load_mnist, load_cifar100\n",
    "from data.classes import cifar10_classes, mnist_classes, intel_image_classes, cifar100_classes\n",
    "\n",
    "from models.models import resnet50, wideresnet2810, vgg16, inceptionv3, efficientnetb2\n",
    "from models.pretrained_models import pretrained_resnet50, pretrained_vgg16\n",
    "\n",
    "from rscnn_functions.budgeting import train_embeddings, fit_gmm, ellipse, overlaps\n",
    "from rscnn_functions.bf_encoding_gt import groundtruthmod\n",
    "from rscnn_functions.belief_mass_betp import belief_to_mass, mass_coeff, final_betp\n",
    "from rscnn_functions.rscnn_loss import BinaryCrossEntropy\n",
    "\n",
    "from utils.train_utils import lr_schedule, train_val_split, data_generator, lr_callbacks, save_model_and_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    num_gpus = len(gpus)\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "    # Set GPUs to use. For example, limit TensorFlow to use 3 GPUs\n",
    "    tf.config.experimental.set_visible_devices(gpus[:3], 'GPU')\n",
    "    \n",
    "# Create a MirroredStrategy for multi-GPU use\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters \n",
    "k = 20  #number of number of non-singleton focal sets \n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = {\"cifar10\": 10, \"mnist\": 10, \"intel_image\": 6, \"cifar100\": 100, \"svhn\": 10, \"fmnist\": 10, \"kmnist\":10}\n",
    "\n",
    "dataset_loader = {\n",
    " \"cifar10\": load_cifar10, \n",
    " \"mnist\": load_mnist, \n",
    " \"intel_image\": load_intel_image, \n",
    " \"cifar100\": load_cifar100, \n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"resnet50\": resnet50, \n",
    "    \"wideresnet_28_10\": wideresnet2810, \n",
    "    \"vgg16\": vgg16,\n",
    "    \"inception_v3\": inceptionv3,\n",
    "    \"efficientnet_b2\": efficientnetb2\n",
    "}\n",
    "\n",
    "pretrained_models = {\n",
    "    \"pretrained_resnet50\": pretrained_resnet50, \n",
    "    \"pretrained_vgg16\": pretrained_vgg16,\n",
    "}\n",
    "\n",
    "class_list_functions = {\n",
    "    \"cifar10\": cifar10_classes,\n",
    "     \"mnist\": mnist_classes, \n",
    "    \"intel_image\": intel_image_classes, \n",
    "    \"cifar100\": cifar100_classes, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_train: (40000, 32, 32, 3)\n",
      "Shape of x_test: (10000, 32, 32, 3)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Define configurations\n",
    "selected_dataset = \"cifar10\"  # Choose the dataset\n",
    "selected_model = \"resnet50\"   # Choose the model\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "# Class list\n",
    "classes = class_list_functions[selected_dataset]()\n",
    "print(\"Classes:\", classes)\n",
    "\n",
    "num_clusters = len(classes)\n",
    "classes_dict = {c:num for c,num in zip(classes, range(len(classes)))}\n",
    "classes_dict_inverse = {num:c for c,num in zip(classes, range(len(classes)))}\n",
    "\n",
    "# Load dataset based on selected_dataset\n",
    "x_train, y_train, x_test_org, x_test, y_test = dataset_loader[selected_dataset]()\n",
    "\n",
    "# Infer input_shape based on selected_dataset\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Train-validation split\n",
    "x_train, y_train, y_train_one_hot, x_val, y_val, y_val_one_hot = train_val_split(x_train, y_train, num_classes[selected_dataset], val_samples=-10000)\n",
    "\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "print(\"Shape of x_test:\", x_test.shape)\n",
    "print(\"Shape of x_val:\", x_val.shape)\n",
    "\n",
    "# Learning rate scheduler\n",
    "callbacks = lr_callbacks(lr_schedule)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = data_generator(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2  (None, 224, 224, 3)       0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d (  (None, 2048)              0         \n",
      " GlobalAveragePooling2D)                                         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " classification (Dense)      (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26215818 (100.01 MB)\n",
      "Trainable params: 26162698 (99.80 MB)\n",
      "Non-trainable params: 53120 (207.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Multi-GPU run\n",
    "with strategy.scope():      \n",
    "    # Create the model based on selected_model\n",
    "    if selected_model in pretrained_models:\n",
    "        model = pretrained_models[selected_model](input_shape=input_shape,  num_classes=num_classes[selected_dataset], final_activation='softmax')\n",
    "    else:\n",
    "        model = models[selected_model](input_shape=input_shape, num_classes=num_classes[selected_dataset], final_activation='softmax')\n",
    "\n",
    "    # Compile the model \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer=\"adam\",\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 40s 128ms/step - loss: 1.0915 - accuracy: 0.6101 - val_loss: 2.6155 - val_accuracy: 0.3965\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(datagen.flow(x_train, y_train_one_hot, batch_size=batch_size),\n",
    "                    validation_data=(x_val, y_val_one_hot),\n",
    "                    epochs=1, verbose=1, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model and weights\n",
    "# save_model_and_weights(model, selected_model, selected_dataset, model_type='CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUDGETING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 25s 77ms/step\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Extracting features from the penultimate layer\n",
    "aux_model = Model(model.input, model.layers[-2].output)\n",
    "\n",
    "# 3D feature space respresentation of class embeddings\n",
    "train_embedded_tsne = train_embeddings(aux_model, x_train, batch_size)\n",
    "\n",
    "# Fitting Gaussian Mixture Models (GMM) to individual classes\n",
    "individual_gms = fit_gmm(classes, train_embedded_tsne, y_train)\n",
    "\n",
    "# Calculating clusters for each class\n",
    "regions, means, max_len = ellipse(individual_gms, num_classes[selected_dataset])\n",
    "\n",
    "# Compute the overlap and choose the sets of classes with highest overlap\n",
    "new_classes = overlaps(k, classes, num_clusters, classes_dict, regions, means, max_len)\n",
    "\n",
    "# np.save('new_classes.npy', new_classes)\n",
    "print(new_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes = np.load('new_classes.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belief-encoding of the ground truth\n",
    "y_train_modified = groundtruthmod(y_train, classes, new_classes, classes_dict_inverse)\n",
    "y_val_modified = groundtruthmod(y_val, classes, new_classes, classes_dict_inverse)\n",
    "y_test_modified = groundtruthmod(y_test, classes, new_classes, classes_dict_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSamplin  (None, 224, 224, 3)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_average_pooling2d_1  (None, 2048)              0         \n",
      "  (GlobalAveragePooling2D)                                       \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " classification (Dense)      (None, 30)                15390     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26226078 (100.04 MB)\n",
      "Trainable params: 26172958 (99.84 MB)\n",
      "Non-trainable params: 53120 (207.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Multi-GPU run\n",
    "with strategy.scope():      \n",
    "    # Create the model based on selected_model\n",
    "    if selected_model in pretrained_models:\n",
    "        new_model = pretrained_models[selected_model](input_shape=input_shape,  num_classes=len(new_classes), final_activation='sigmoid')\n",
    "    else:\n",
    "        new_model = models[selected_model](input_shape=input_shape, num_classes=len(new_classes), final_activation='sigmoid')\n",
    "\n",
    "    # Compile the model \n",
    "    new_model.compile(loss=BinaryCrossEntropy,\n",
    "                optimizer=\"adam\",\n",
    "                metrics=['binary_accuracy'])\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 03:53:54.567004: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\017TensorDataset:0\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/100\n",
      "INFO:tensorflow:Collective all_reduce tensors: 218 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n",
      "INFO:tensorflow:Collective all_reduce tensors: 218 all_reduces, num_devices = 8, group_size = 8, implementation = CommunicationImplementation.NCCL, num_packs = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 03:55:52.780674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-26 03:55:53.276082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-26 03:55:53.809108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-26 03:55:54.012490: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:55:54.013763: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 7.5\n",
      "2023-11-26 03:55:54.013773: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
      "2023-11-26 03:55:54.013818: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-11-26 03:55:54.298428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-26 03:55:54.870759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-26 03:55:55.099082: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:55:55.408231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-26 03:55:55.878509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-26 03:55:56.133236: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:55:56.202178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2023-11-26 03:55:56.618935: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:55:57.080481: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:55:57.542426: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:55:57.994485: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:55:58.409139: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:11.822393: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f20197beca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-26 03:56:11.822433: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Quadro RTX 6000, Compute Capability 7.5\n",
      "2023-11-26 03:56:11.822439: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Quadro RTX 6000, Compute Capability 7.5\n",
      "2023-11-26 03:56:11.822443: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (2): Quadro RTX 6000, Compute Capability 7.5\n",
      "2023-11-26 03:56:11.822446: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (3): Quadro RTX 6000, Compute Capability 7.5\n",
      "2023-11-26 03:56:11.822450: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (4): Quadro RTX 6000, Compute Capability 7.5\n",
      "2023-11-26 03:56:11.822454: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (5): Quadro RTX 6000, Compute Capability 7.5\n",
      "2023-11-26 03:56:11.822458: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (6): Quadro RTX 6000, Compute Capability 7.5\n",
      "2023-11-26 03:56:11.822462: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (7): Quadro RTX 6000, Compute Capability 7.5\n",
      "2023-11-26 03:56:11.833992: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-26 03:56:11.898098: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:11.898136: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:11.898177: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:11.900965: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:11.901072: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:11.915556: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2023-11-26 03:56:11.977324: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:11.977437: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:24.344382: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:24.446161: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:24.542264: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:24.641724: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "2023-11-26 03:56:24.862670: E tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:114] *** WARNING *** You are using ptxas 9.1.108, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 215s 195ms/step - loss: 0.3934 - binary_accuracy: 0.8354 - val_loss: 0.4663 - val_binary_accuracy: 0.8214 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.3224 - binary_accuracy: 0.8591 - val_loss: 0.5424 - val_binary_accuracy: 0.8241 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.2729 - binary_accuracy: 0.8831 - val_loss: 0.4391 - val_binary_accuracy: 0.8449 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.2411 - binary_accuracy: 0.8979 - val_loss: 0.3745 - val_binary_accuracy: 0.8713 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 43s 138ms/step - loss: 0.2134 - binary_accuracy: 0.9109 - val_loss: 0.4912 - val_binary_accuracy: 0.8473 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.1914 - binary_accuracy: 0.9209 - val_loss: 0.2009 - val_binary_accuracy: 0.9178 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.1773 - binary_accuracy: 0.9270 - val_loss: 0.1818 - val_binary_accuracy: 0.9273 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.1630 - binary_accuracy: 0.9334 - val_loss: 0.2317 - val_binary_accuracy: 0.9104 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.1515 - binary_accuracy: 0.9380 - val_loss: 0.2805 - val_binary_accuracy: 0.9059 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.1406 - binary_accuracy: 0.9430 - val_loss: 0.1792 - val_binary_accuracy: 0.9300 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.1336 - binary_accuracy: 0.9460 - val_loss: 0.1509 - val_binary_accuracy: 0.9392 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.1248 - binary_accuracy: 0.9503 - val_loss: 0.1711 - val_binary_accuracy: 0.9315 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.1184 - binary_accuracy: 0.9523 - val_loss: 0.1651 - val_binary_accuracy: 0.9343 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.1143 - binary_accuracy: 0.9543 - val_loss: 0.1499 - val_binary_accuracy: 0.9410 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.1078 - binary_accuracy: 0.9572 - val_loss: 0.1246 - val_binary_accuracy: 0.9513 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.1032 - binary_accuracy: 0.9593 - val_loss: 0.1285 - val_binary_accuracy: 0.9482 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.0995 - binary_accuracy: 0.9606 - val_loss: 0.1309 - val_binary_accuracy: 0.9503 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0948 - binary_accuracy: 0.9626 - val_loss: 0.1140 - val_binary_accuracy: 0.9539 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0896 - binary_accuracy: 0.9648 - val_loss: 0.1230 - val_binary_accuracy: 0.9528 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0858 - binary_accuracy: 0.9663 - val_loss: 0.1423 - val_binary_accuracy: 0.9456 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0834 - binary_accuracy: 0.9668 - val_loss: 0.1325 - val_binary_accuracy: 0.9506 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0803 - binary_accuracy: 0.9685 - val_loss: 0.1317 - val_binary_accuracy: 0.9529 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0746 - binary_accuracy: 0.9707 - val_loss: 0.1134 - val_binary_accuracy: 0.9580 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0740 - binary_accuracy: 0.9714 - val_loss: 0.1032 - val_binary_accuracy: 0.9598 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0695 - binary_accuracy: 0.9728 - val_loss: 0.1054 - val_binary_accuracy: 0.9598 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0668 - binary_accuracy: 0.9739 - val_loss: 0.1067 - val_binary_accuracy: 0.9617 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0656 - binary_accuracy: 0.9743 - val_loss: 0.1009 - val_binary_accuracy: 0.9625 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0622 - binary_accuracy: 0.9759 - val_loss: 0.1355 - val_binary_accuracy: 0.9518 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 43s 138ms/step - loss: 0.0593 - binary_accuracy: 0.9772 - val_loss: 0.1070 - val_binary_accuracy: 0.9607 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0587 - binary_accuracy: 0.9773 - val_loss: 0.1034 - val_binary_accuracy: 0.9613 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0562 - binary_accuracy: 0.9782 - val_loss: 0.1087 - val_binary_accuracy: 0.9628 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.0533 - binary_accuracy: 0.9793 - val_loss: 0.1387 - val_binary_accuracy: 0.9531 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.0513 - binary_accuracy: 0.9803 - val_loss: 0.1059 - val_binary_accuracy: 0.9645 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.0480 - binary_accuracy: 0.9816 - val_loss: 0.1134 - val_binary_accuracy: 0.9605 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0474 - binary_accuracy: 0.9821 - val_loss: 0.0994 - val_binary_accuracy: 0.9650 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0458 - binary_accuracy: 0.9826 - val_loss: 0.1017 - val_binary_accuracy: 0.9654 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0427 - binary_accuracy: 0.9835 - val_loss: 0.1109 - val_binary_accuracy: 0.9618 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 46s 146ms/step - loss: 0.0432 - binary_accuracy: 0.9836 - val_loss: 0.0937 - val_binary_accuracy: 0.9668 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 45s 142ms/step - loss: 0.0411 - binary_accuracy: 0.9842 - val_loss: 0.1107 - val_binary_accuracy: 0.9644 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 45s 142ms/step - loss: 0.0395 - binary_accuracy: 0.9851 - val_loss: 0.0907 - val_binary_accuracy: 0.9696 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0382 - binary_accuracy: 0.9853 - val_loss: 0.1005 - val_binary_accuracy: 0.9649 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0366 - binary_accuracy: 0.9862 - val_loss: 0.1253 - val_binary_accuracy: 0.9603 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0344 - binary_accuracy: 0.9872 - val_loss: 0.1119 - val_binary_accuracy: 0.9629 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0350 - binary_accuracy: 0.9868 - val_loss: 0.0843 - val_binary_accuracy: 0.9705 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0338 - binary_accuracy: 0.9872 - val_loss: 0.1092 - val_binary_accuracy: 0.9651 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0310 - binary_accuracy: 0.9883 - val_loss: 0.1178 - val_binary_accuracy: 0.9624 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0304 - binary_accuracy: 0.9886 - val_loss: 0.0975 - val_binary_accuracy: 0.9687 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0285 - binary_accuracy: 0.9892 - val_loss: 0.1475 - val_binary_accuracy: 0.9604 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0286 - binary_accuracy: 0.9893 - val_loss: 0.1073 - val_binary_accuracy: 0.9662 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0284 - binary_accuracy: 0.9893 - val_loss: 0.0895 - val_binary_accuracy: 0.9713 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0264 - binary_accuracy: 0.9905 - val_loss: 0.0981 - val_binary_accuracy: 0.9699 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0263 - binary_accuracy: 0.9901 - val_loss: 0.0856 - val_binary_accuracy: 0.9732 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0248 - binary_accuracy: 0.9909 - val_loss: 0.1115 - val_binary_accuracy: 0.9671 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0246 - binary_accuracy: 0.9908 - val_loss: 0.0965 - val_binary_accuracy: 0.9716 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0245 - binary_accuracy: 0.9907 - val_loss: 0.1060 - val_binary_accuracy: 0.9673 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.0225 - binary_accuracy: 0.9917 - val_loss: 0.0944 - val_binary_accuracy: 0.9717 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0226 - binary_accuracy: 0.9917 - val_loss: 0.0830 - val_binary_accuracy: 0.9734 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0214 - binary_accuracy: 0.9921 - val_loss: 0.1017 - val_binary_accuracy: 0.9696 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 59/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0223 - binary_accuracy: 0.9916 - val_loss: 0.1008 - val_binary_accuracy: 0.9696 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.0198 - binary_accuracy: 0.9927 - val_loss: 0.1161 - val_binary_accuracy: 0.9664 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0207 - binary_accuracy: 0.9923 - val_loss: 0.1116 - val_binary_accuracy: 0.9684 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0192 - binary_accuracy: 0.9932 - val_loss: 0.0994 - val_binary_accuracy: 0.9721 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0194 - binary_accuracy: 0.9930 - val_loss: 0.1099 - val_binary_accuracy: 0.9707 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0203 - binary_accuracy: 0.9927 - val_loss: 0.1002 - val_binary_accuracy: 0.9692 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0176 - binary_accuracy: 0.9936 - val_loss: 0.0951 - val_binary_accuracy: 0.9727 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0171 - binary_accuracy: 0.9938 - val_loss: 0.0975 - val_binary_accuracy: 0.9730 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0172 - binary_accuracy: 0.9938 - val_loss: 0.0999 - val_binary_accuracy: 0.9706 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0171 - binary_accuracy: 0.9937 - val_loss: 0.1143 - val_binary_accuracy: 0.9695 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0159 - binary_accuracy: 0.9941 - val_loss: 0.1185 - val_binary_accuracy: 0.9692 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0165 - binary_accuracy: 0.9937 - val_loss: 0.0998 - val_binary_accuracy: 0.9729 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 71/100\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.0157 - binary_accuracy: 0.9943 - val_loss: 0.1100 - val_binary_accuracy: 0.9721 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0165 - binary_accuracy: 0.9940 - val_loss: 0.1152 - val_binary_accuracy: 0.9686 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0154 - binary_accuracy: 0.9944 - val_loss: 0.0958 - val_binary_accuracy: 0.9728 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0149 - binary_accuracy: 0.9945 - val_loss: 0.1014 - val_binary_accuracy: 0.9724 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 43s 138ms/step - loss: 0.0154 - binary_accuracy: 0.9946 - val_loss: 0.1203 - val_binary_accuracy: 0.9673 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0139 - binary_accuracy: 0.9949 - val_loss: 0.0988 - val_binary_accuracy: 0.9724 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0134 - binary_accuracy: 0.9953 - val_loss: 0.1063 - val_binary_accuracy: 0.9719 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0145 - binary_accuracy: 0.9945 - val_loss: 0.1226 - val_binary_accuracy: 0.9671 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0131 - binary_accuracy: 0.9952 - val_loss: 0.0988 - val_binary_accuracy: 0.9732 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0135 - binary_accuracy: 0.9950 - val_loss: 0.1053 - val_binary_accuracy: 0.9712 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 46s 146ms/step - loss: 0.0130 - binary_accuracy: 0.9954 - val_loss: 0.1041 - val_binary_accuracy: 0.9729 - lr: 0.0010\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 45s 143ms/step - loss: 0.0060 - binary_accuracy: 0.9979 - val_loss: 0.0879 - val_binary_accuracy: 0.9772 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 45s 143ms/step - loss: 0.0036 - binary_accuracy: 0.9988 - val_loss: 0.0897 - val_binary_accuracy: 0.9778 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 45s 142ms/step - loss: 0.0033 - binary_accuracy: 0.9988 - val_loss: 0.0905 - val_binary_accuracy: 0.9780 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0024 - binary_accuracy: 0.9992 - val_loss: 0.0959 - val_binary_accuracy: 0.9778 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0024 - binary_accuracy: 0.9992 - val_loss: 0.0943 - val_binary_accuracy: 0.9784 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0025 - binary_accuracy: 0.9992 - val_loss: 0.0974 - val_binary_accuracy: 0.9785 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0020 - binary_accuracy: 0.9993 - val_loss: 0.0963 - val_binary_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0018 - binary_accuracy: 0.9994 - val_loss: 0.1011 - val_binary_accuracy: 0.9782 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0020 - binary_accuracy: 0.9993 - val_loss: 0.1021 - val_binary_accuracy: 0.9781 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0014 - binary_accuracy: 0.9995 - val_loss: 0.1028 - val_binary_accuracy: 0.9785 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0018 - binary_accuracy: 0.9994 - val_loss: 0.1020 - val_binary_accuracy: 0.9790 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0018 - binary_accuracy: 0.9994 - val_loss: 0.1022 - val_binary_accuracy: 0.9786 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0015 - binary_accuracy: 0.9995 - val_loss: 0.1048 - val_binary_accuracy: 0.9786 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0011 - binary_accuracy: 0.9996 - val_loss: 0.1049 - val_binary_accuracy: 0.9788 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0012 - binary_accuracy: 0.9995 - val_loss: 0.1055 - val_binary_accuracy: 0.9788 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0011 - binary_accuracy: 0.9996 - val_loss: 0.1089 - val_binary_accuracy: 0.9786 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0013 - binary_accuracy: 0.9996 - val_loss: 0.1085 - val_binary_accuracy: 0.9786 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0015 - binary_accuracy: 0.9995 - val_loss: 0.1065 - val_binary_accuracy: 0.9789 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/100\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.0013 - binary_accuracy: 0.9996 - val_loss: 0.1078 - val_binary_accuracy: 0.9792 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history_new = new_model.fit(datagen.flow(x_train, y_train_modified, batch_size=batch_size),\n",
    "                    validation_data=(x_val, y_val_modified),\n",
    "                    epochs=epochs, verbose=1, workers=2,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 05:10:21.002969: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorDataset/_1\"\n",
      "op: \"TensorDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 1\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\022TensorDataset:5754\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 1/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0287 - binary_accuracy: 0.9898 - val_loss: 0.1183 - val_binary_accuracy: 0.9673 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 2/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0191 - binary_accuracy: 0.9931 - val_loss: 0.0984 - val_binary_accuracy: 0.9742 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 3/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0150 - binary_accuracy: 0.9945 - val_loss: 0.1113 - val_binary_accuracy: 0.9710 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 4/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0146 - binary_accuracy: 0.9948 - val_loss: 0.1255 - val_binary_accuracy: 0.9682 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 5/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0126 - binary_accuracy: 0.9954 - val_loss: 0.1114 - val_binary_accuracy: 0.9705 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 6/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0124 - binary_accuracy: 0.9956 - val_loss: 0.0995 - val_binary_accuracy: 0.9728 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 7/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0114 - binary_accuracy: 0.9960 - val_loss: 0.1103 - val_binary_accuracy: 0.9729 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 8/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0112 - binary_accuracy: 0.9961 - val_loss: 0.1148 - val_binary_accuracy: 0.9708 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 9/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0119 - binary_accuracy: 0.9957 - val_loss: 0.1087 - val_binary_accuracy: 0.9711 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 10/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0109 - binary_accuracy: 0.9959 - val_loss: 0.1446 - val_binary_accuracy: 0.9644 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 11/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0107 - binary_accuracy: 0.9962 - val_loss: 0.1084 - val_binary_accuracy: 0.9705 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 12/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0092 - binary_accuracy: 0.9967 - val_loss: 0.1265 - val_binary_accuracy: 0.9678 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 13/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0115 - binary_accuracy: 0.9961 - val_loss: 0.1338 - val_binary_accuracy: 0.9685 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 14/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0104 - binary_accuracy: 0.9962 - val_loss: 0.1064 - val_binary_accuracy: 0.9721 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 15/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0095 - binary_accuracy: 0.9967 - val_loss: 0.1064 - val_binary_accuracy: 0.9739 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 16/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0096 - binary_accuracy: 0.9966 - val_loss: 0.0992 - val_binary_accuracy: 0.9744 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 17/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0103 - binary_accuracy: 0.9965 - val_loss: 0.1081 - val_binary_accuracy: 0.9735 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 18/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0096 - binary_accuracy: 0.9966 - val_loss: 0.1494 - val_binary_accuracy: 0.9661 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 19/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0092 - binary_accuracy: 0.9968 - val_loss: 0.1238 - val_binary_accuracy: 0.9685 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 20/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0093 - binary_accuracy: 0.9968 - val_loss: 0.1079 - val_binary_accuracy: 0.9745 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 21/100\n",
      "313/313 [==============================] - 47s 148ms/step - loss: 0.0099 - binary_accuracy: 0.9965 - val_loss: 0.1019 - val_binary_accuracy: 0.9738 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 22/100\n",
      "313/313 [==============================] - 45s 143ms/step - loss: 0.0088 - binary_accuracy: 0.9969 - val_loss: 0.1147 - val_binary_accuracy: 0.9720 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 23/100\n",
      "313/313 [==============================] - 45s 143ms/step - loss: 0.0090 - binary_accuracy: 0.9967 - val_loss: 0.1070 - val_binary_accuracy: 0.9743 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 24/100\n",
      "313/313 [==============================] - 44s 142ms/step - loss: 0.0083 - binary_accuracy: 0.9970 - val_loss: 0.1045 - val_binary_accuracy: 0.9750 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 25/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0095 - binary_accuracy: 0.9967 - val_loss: 0.0952 - val_binary_accuracy: 0.9747 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 26/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0093 - binary_accuracy: 0.9968 - val_loss: 0.0993 - val_binary_accuracy: 0.9736 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 27/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0081 - binary_accuracy: 0.9973 - val_loss: 0.1318 - val_binary_accuracy: 0.9704 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 28/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0082 - binary_accuracy: 0.9972 - val_loss: 0.1088 - val_binary_accuracy: 0.9712 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 29/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0084 - binary_accuracy: 0.9970 - val_loss: 0.1368 - val_binary_accuracy: 0.9689 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 30/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0085 - binary_accuracy: 0.9969 - val_loss: 0.1157 - val_binary_accuracy: 0.9721 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 31/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0072 - binary_accuracy: 0.9975 - val_loss: 0.1199 - val_binary_accuracy: 0.9735 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 32/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0081 - binary_accuracy: 0.9974 - val_loss: 0.0989 - val_binary_accuracy: 0.9748 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 33/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0071 - binary_accuracy: 0.9975 - val_loss: 0.1177 - val_binary_accuracy: 0.9726 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 34/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0077 - binary_accuracy: 0.9973 - val_loss: 0.1191 - val_binary_accuracy: 0.9726 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 35/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0081 - binary_accuracy: 0.9971 - val_loss: 0.1074 - val_binary_accuracy: 0.9734 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 36/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0073 - binary_accuracy: 0.9975 - val_loss: 0.1298 - val_binary_accuracy: 0.9682 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 37/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0071 - binary_accuracy: 0.9974 - val_loss: 0.1439 - val_binary_accuracy: 0.9695 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 38/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0078 - binary_accuracy: 0.9973 - val_loss: 0.1128 - val_binary_accuracy: 0.9734 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 39/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0065 - binary_accuracy: 0.9977 - val_loss: 0.1022 - val_binary_accuracy: 0.9753 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 40/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0075 - binary_accuracy: 0.9975 - val_loss: 0.1144 - val_binary_accuracy: 0.9710 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 41/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0071 - binary_accuracy: 0.9975 - val_loss: 0.1044 - val_binary_accuracy: 0.9751 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 42/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0074 - binary_accuracy: 0.9974 - val_loss: 0.1085 - val_binary_accuracy: 0.9741 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 43/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0067 - binary_accuracy: 0.9978 - val_loss: 0.1305 - val_binary_accuracy: 0.9704 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 44/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0071 - binary_accuracy: 0.9976 - val_loss: 0.1014 - val_binary_accuracy: 0.9747 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 45/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0074 - binary_accuracy: 0.9974 - val_loss: 0.1374 - val_binary_accuracy: 0.9704 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 46/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0076 - binary_accuracy: 0.9972 - val_loss: 0.1116 - val_binary_accuracy: 0.9727 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 47/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0065 - binary_accuracy: 0.9977 - val_loss: 0.1205 - val_binary_accuracy: 0.9721 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 48/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0062 - binary_accuracy: 0.9978 - val_loss: 0.1127 - val_binary_accuracy: 0.9742 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 49/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0060 - binary_accuracy: 0.9980 - val_loss: 0.1375 - val_binary_accuracy: 0.9702 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 50/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0069 - binary_accuracy: 0.9977 - val_loss: 0.1133 - val_binary_accuracy: 0.9738 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 51/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0061 - binary_accuracy: 0.9980 - val_loss: 0.1159 - val_binary_accuracy: 0.9738 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 52/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0056 - binary_accuracy: 0.9981 - val_loss: 0.1049 - val_binary_accuracy: 0.9742 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 53/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0066 - binary_accuracy: 0.9976 - val_loss: 0.1309 - val_binary_accuracy: 0.9701 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 54/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0074 - binary_accuracy: 0.9974 - val_loss: 0.1314 - val_binary_accuracy: 0.9716 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 55/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0061 - binary_accuracy: 0.9979 - val_loss: 0.1183 - val_binary_accuracy: 0.9725 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 56/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0062 - binary_accuracy: 0.9979 - val_loss: 0.1084 - val_binary_accuracy: 0.9752 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 57/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0071 - binary_accuracy: 0.9975 - val_loss: 0.1119 - val_binary_accuracy: 0.9722 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 58/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0054 - binary_accuracy: 0.9981 - val_loss: 0.1097 - val_binary_accuracy: 0.9756 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 59/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0060 - binary_accuracy: 0.9979 - val_loss: 0.1297 - val_binary_accuracy: 0.9717 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 60/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0068 - binary_accuracy: 0.9976 - val_loss: 0.1120 - val_binary_accuracy: 0.9740 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 61/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0052 - binary_accuracy: 0.9981 - val_loss: 0.1074 - val_binary_accuracy: 0.9749 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 62/100\n",
      "313/313 [==============================] - 45s 143ms/step - loss: 0.0053 - binary_accuracy: 0.9982 - val_loss: 0.1157 - val_binary_accuracy: 0.9736 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 63/100\n",
      "313/313 [==============================] - 45s 143ms/step - loss: 0.0060 - binary_accuracy: 0.9979 - val_loss: 0.1060 - val_binary_accuracy: 0.9757 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 64/100\n",
      "313/313 [==============================] - 45s 142ms/step - loss: 0.0069 - binary_accuracy: 0.9976 - val_loss: 0.1067 - val_binary_accuracy: 0.9746 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 65/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0048 - binary_accuracy: 0.9983 - val_loss: 0.1125 - val_binary_accuracy: 0.9758 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 66/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0055 - binary_accuracy: 0.9981 - val_loss: 0.1347 - val_binary_accuracy: 0.9692 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 67/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0054 - binary_accuracy: 0.9981 - val_loss: 0.1204 - val_binary_accuracy: 0.9738 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 68/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0061 - binary_accuracy: 0.9979 - val_loss: 0.1045 - val_binary_accuracy: 0.9754 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 69/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0053 - binary_accuracy: 0.9981 - val_loss: 0.1163 - val_binary_accuracy: 0.9748 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 70/100\n",
      "313/313 [==============================] - 44s 141ms/step - loss: 0.0055 - binary_accuracy: 0.9980 - val_loss: 0.1148 - val_binary_accuracy: 0.9725 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 71/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0065 - binary_accuracy: 0.9977 - val_loss: 0.1325 - val_binary_accuracy: 0.9712 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 72/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0054 - binary_accuracy: 0.9981 - val_loss: 0.1345 - val_binary_accuracy: 0.9720 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 73/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0053 - binary_accuracy: 0.9982 - val_loss: 0.1206 - val_binary_accuracy: 0.9738 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 74/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0046 - binary_accuracy: 0.9984 - val_loss: 0.1085 - val_binary_accuracy: 0.9746 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 75/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0051 - binary_accuracy: 0.9983 - val_loss: 0.1319 - val_binary_accuracy: 0.9723 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 76/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0057 - binary_accuracy: 0.9980 - val_loss: 0.1193 - val_binary_accuracy: 0.9723 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 77/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0052 - binary_accuracy: 0.9982 - val_loss: 0.1132 - val_binary_accuracy: 0.9754 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 78/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0048 - binary_accuracy: 0.9984 - val_loss: 0.1028 - val_binary_accuracy: 0.9767 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 79/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0060 - binary_accuracy: 0.9979 - val_loss: 0.1169 - val_binary_accuracy: 0.9742 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 80/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0049 - binary_accuracy: 0.9985 - val_loss: 0.1066 - val_binary_accuracy: 0.9760 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 81/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0045 - binary_accuracy: 0.9985 - val_loss: 0.1309 - val_binary_accuracy: 0.9728 - lr: 0.0010\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 0.0021 - binary_accuracy: 0.9994 - val_loss: 0.1005 - val_binary_accuracy: 0.9785 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0012 - binary_accuracy: 0.9996 - val_loss: 0.0989 - val_binary_accuracy: 0.9785 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 0.0012 - binary_accuracy: 0.9996 - val_loss: 0.1003 - val_binary_accuracy: 0.9783 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 9.6410e-04 - binary_accuracy: 0.9997 - val_loss: 0.0988 - val_binary_accuracy: 0.9792 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 7.3607e-04 - binary_accuracy: 0.9998 - val_loss: 0.1018 - val_binary_accuracy: 0.9793 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 6.7800e-04 - binary_accuracy: 0.9998 - val_loss: 0.1023 - val_binary_accuracy: 0.9795 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 6.8911e-04 - binary_accuracy: 0.9998 - val_loss: 0.1076 - val_binary_accuracy: 0.9790 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 5.3013e-04 - binary_accuracy: 0.9999 - val_loss: 0.1080 - val_binary_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 4.6383e-04 - binary_accuracy: 0.9999 - val_loss: 0.1072 - val_binary_accuracy: 0.9794 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 4.4912e-04 - binary_accuracy: 0.9998 - val_loss: 0.1104 - val_binary_accuracy: 0.9795 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 4.3982e-04 - binary_accuracy: 0.9999 - val_loss: 0.1139 - val_binary_accuracy: 0.9792 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 4.3904e-04 - binary_accuracy: 0.9999 - val_loss: 0.1140 - val_binary_accuracy: 0.9793 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 3.8660e-04 - binary_accuracy: 0.9999 - val_loss: 0.1170 - val_binary_accuracy: 0.9789 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 5.2621e-04 - binary_accuracy: 0.9998 - val_loss: 0.1155 - val_binary_accuracy: 0.9797 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 5.5015e-04 - binary_accuracy: 0.9998 - val_loss: 0.1131 - val_binary_accuracy: 0.9797 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/100\n",
      "313/313 [==============================] - 44s 140ms/step - loss: 3.1725e-04 - binary_accuracy: 0.9999 - val_loss: 0.1167 - val_binary_accuracy: 0.9795 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 4.6172e-04 - binary_accuracy: 0.9998 - val_loss: 0.1149 - val_binary_accuracy: 0.9798 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 3.7853e-04 - binary_accuracy: 0.9998 - val_loss: 0.1169 - val_binary_accuracy: 0.9795 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/100\n",
      "313/313 [==============================] - 44s 139ms/step - loss: 3.8204e-04 - binary_accuracy: 0.9999 - val_loss: 0.1178 - val_binary_accuracy: 0.9795 - lr: 3.1623e-05\n"
     ]
    }
   ],
   "source": [
    "history_new = new_model.fit(datagen.flow(x_train, y_train_modified, batch_size=batch_size),\n",
    "                    validation_data=(x_val, y_val_modified),\n",
    "                    epochs=epochs, verbose=1, workers=2,\n",
    "                   callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model and weights\n",
    "# save_model_and_weights(new_model, selected_model, selected_dataset, model_type='RSCNN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
